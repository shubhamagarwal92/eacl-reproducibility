bibtex,Authors,Title,Venue,Year,Abbr venue,Abstract,Paper Summary,Definitions of reproducibility or replicability etc?,"Notes, e.g. findings, important points, etc. not made in abstract",Reproducible / Replicable according to authors? ,Links,Type of paper (Short / Long),
"@inproceedings{schwartz-2010-reproducible,
    title = ""Reproducible Results in Parsing-Based Machine Translation: The {JHU} Shared Task Submission"",
    author = ""Schwartz, Lane"",
    booktitle = ""Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR}"",
    month = jul,
    year = ""2010"",
    address = ""Uppsala, Sweden"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W10-1726"",
    pages = ""177--182"",
    abstract = ""We  present  the  Johns  Hopkins  University submission to the 2010 WMT shared translation  task.   We  describe  processing steps  using  open  data  and  open  source software used in our submission, and provide  the  scripts  and  configurations  re-quired to train, tune, and test our machine translation system.""
}
","Schwartz, Lane",Reproducible Results in Parsing-Based Machine Translation: The {JHU} Shared Task Submission,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},2010,WS,"We  present  the  Johns  Hopkins  University submission to the 2010 WMT shared translation  task.   We  describe  processing steps  using  open  data  and  open  source software used in our submission, and provide  the  scripts  and  configurations  re-quired to train, tune, and test our machine translation system.","1. one sentence summary of what paper is about
Presents methods and results for JHU submission to WMT'10, also discussing and addressing reproducibility of systems and results.
2. paper's central goal/question/hypothesis
Twofold: (1) to build WMT'10 system using only open-access tools and data; (2) role-modelling reporting and resource sharing designed to support full recreation of systems.
3. methodology
(1) builds MT systems for multiple language pairs using only open-access tools and data; (2) meticulously documents all steps and tools used and makes all code and data available via sourceforge.
4. reported results
WMT'10 and WMT'09 results, with their WMT'10 German-English system ranking top in terms of TER.
5. paper's conclusions and/or main contributions
No explicit conclusions. Early example of diagnosing the reproducibility problem in NLP systems, plus demonstrates how full system recreation can be achieved through code and data sharing.
",,,,,,
"@inproceedings{fokkens-etal-2013-offspring,
    title = ""Offspring from Reproduction Problems: What Replication Failure Teaches Us"",
    author = ""Fokkens, Antske  and
      van Erp, Marieke  and
      Postma, Marten  and
      Pedersen, Ted  and
      Vossen, Piek  and
      Freire, Nuno"",
    booktitle = ""Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2013"",
    address = ""Sofia, Bulgaria"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/P13-1166"",
    pages = ""1691--1701"",
    abstract = ""Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing  work.   We  present  two  concrete  use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult.  We show that the deviation that can be found in reproduction  efforts  leads  to  questions  about how  our  results  should  be  interpreted. Moreover,  investigating  these  deviations provides new insights and a deeper understanding of the examined techniques.  We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers.  Our use cases show that these aspects may change the  answer  to  research  questions  leading us  to  conclude  that  more  care  should  betaken in interpreting our results and more research  involving  systematic  testing  of methods is required in our field.""
}
","Fokkens, Antske  and
      van Erp, Marieke  and
      Postma, Marten  and
      Pedersen, Ted  and
      Vossen, Piek  and
      Freire, Nuno",Offspring from Reproduction Problems: What Replication Failure Teaches Us,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2013,ACL,"Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing  work.   We  present  two  concrete  use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult.  We show that the deviation that can be found in reproduction  efforts  leads  to  questions  about how  our  results  should  be  interpreted. Moreover,  investigating  these  deviations provides new insights and a deeper understanding of the examined techniques.  We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers.  Our use cases show that these aspects may change the  answer  to  research  questions  leading us  to  conclude  that  more  care  should  betaken in interpreting our results and more research  involving  systematic  testing  of methods is required in our field.","1. one sentence summary of what paper is about
Reports on reproduction attempts of two previous connected results for WordNet similarity measures and one for NER, also systematically investigating the impact of variations in implementational details on scores
2. paper's central goal/question/hypothesis
To carry out systematic reproduction attempts in two NLP applications and examine reasons for failure.
3. methodology
Faithful recreation of original systems with cooperation from original authors; systematic variation of parameters and implementational details to explore differences in results.
4. reported results
Results differed for WNS task in unspecified ways, and were 20 F-score points lower for NER. Systematic exploration of differences failed to make up all the difference, but identified differences that affect the results.
5. paper's conclusions and/or main contributions
* Identifies 5 categories of differences (most of which rarely reported) that can make substantive difference to results: preprocessing, experimental set-up, versioning, system output, and system variation; detailed reproduction research leads to better understanding of techniques
* asks fundamental question: what do our results mean if you can so easily ruin/improve them with seemingly minor differences?",,"* 5 aspects often omitted from reporting but can make big difference to results
* Section 2 gives overview of recent (pre-2013) work on reproducibility etc. in CS
  - 4 cited papers for consensus that 'reproducibility' is about reproducing conclusion (not exact scores/numbers) 
  - 'replication' defined as recreating an experiment faithfully aiming to get the same results (same numbers)
  - source code sharing alone is not enough for reproduction (Mende, 2010; Louridas & Gousios, 2012)
  - Collin's (1999) benchmark parsing results plus Bikel (2004) listing all the other details needed in addition to the info in the original paper in order to reproduce results
  - failed replication attempt on NER 
* tried to reproduce values of similarity measures on WordNet & found they differed even with same software and Wordnet version, hence must be other factors influence outputs; found: different approaches to POS-tag ambiguity resolution, choice of gold standard, resolution of ties in Spearman's calcualtion; found very large maximal differences in Spearman's when systematically varying these factors, hence very different rankings of the measures.
* NER and WordNet similarity both illustrate difficulty in reproducing results
* small differences in implementational choices have large effects on performance measures
",,,,
"@inproceedings{neveol-etal-2016-replicability,
    title = ""Replicability of Research in Biomedical Natural Language Processing: a pilot evaluation for a coding task"",
    author = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Cohen, Kevin  and
      Grouin, Cyril  and
      Robert, Aude"",
    booktitle = ""Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis"",
    month = nov,
    year = ""2016"",
    address = ""Auxtin, TX"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W16-6110"",
    doi = ""10.18653/v1/W16-6110"",
    pages = ""78--84"",
    abstract = ""The  scientific  community  is  facing  raising concerns about the reproducibility of research in many fields.  To address this issue in Natural Language Processing, the CLEF eHealth 2016  lab  offered  a  replication  track  together with the Clinical Information Extraction task. Herein, we report detailed results of the replication experiments carried out with the three systems submitted to the track.  While all results were ultimately replicated, we found that the systems were poorly rated by analysts on documentation  aspects  such  as  ”ease  of  understanding system requirements” (33%) and” provision of information while system is running” (33%).  As a result, simple steps could be taken by system authors to increase the ease of replicability of their work, thereby increasing the ease of reusing the systems.  Our experiments  aim  to  raise  the  awareness  of  the community towards the challenges of replication and community sharing of NLP systems.""
}
","N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Cohen, Kevin  and
      Grouin, Cyril  and
      Robert, Aude",Replicability of Research in Biomedical Natural Language Processing: a pilot evaluation for a coding task,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,2016,WS,"The  scientific  community  is  facing  raising concerns about the reproducibility of research in many fields.  To address this issue in Natural Language Processing, the CLEF eHealth 2016  lab  offered  a  replication  track  together with the Clinical Information Extraction task. Herein, we report detailed results of the replication experiments carried out with the three systems submitted to the track.  While all results were ultimately replicated, we found that the systems were poorly rated by analysts on documentation  aspects  such  as  ”ease  of  understanding system requirements” (33%) and” provision of information while system is running” (33%).  As a result, simple steps could be taken by system authors to increase the ease of replicability of their work, thereby increasing the ease of reusing the systems.  Our experiments  aim  to  raise  the  awareness  of  the community towards the challenges of replication and community sharing of NLP systems.","1. one sentence summary of what paper is about
[text]
2. paper's central goal/question/hypothesis
[text]
3. methodology
[text]
4. reported results
[text]
5. paper's conclusions and/or main contributions
[text]
",,,,,,
"@inproceedings{fares-etal-2017-word,
    title = ""Word vectors, reuse, and replicability: Towards a community repository of large-text resources"",
    author = ""Fares, Murhaf  and
      Kutuzov, Andrey  and
      Oepen, Stephan  and
      Velldal, Erik"",
    booktitle = ""Proceedings of the 21st Nordic Conference on Computational Linguistics"",
    month = may,
    year = ""2017"",
    address = ""Gothenburg, Sweden"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W17-0237"",
    pages = ""271--276"",
    abstract = ""This paper describes an emerging shared repository of large-text resources for creating word vectors, including pre-processed corpora and pre-trained vectors for a range of frameworks and configurations.   This will facilitate reuse, rapid experimentation, and replicability of results.""
}
","Fares, Murhaf  and
      Kutuzov, Andrey  and
      Oepen, Stephan  and
      Velldal, Erik","Word vectors, reuse, and replicability: Towards a community repository of large-text resources",Proceedings of the 21st Nordic Conference on Computational Linguistics,2017,WS,"This paper describes an emerging shared repository of large-text resources for creating word vectors, including pre-processed corpora and pre-trained vectors for a range of frameworks and configurations.   This will facilitate reuse, rapid experimentation, and replicability of results.","1. one sentence summary of what paper is about
Demo paper with the aim of providing shared repository for creating word vectors with a focus on reproducibility of results
2. paper's central goal/question/hypothesis
To create a shared repository of large-text resources for word vectors, including pre-processed corpora and pre-trained vector models to facilitate rapid experimentation and replicability
3. methodology
Trained embeddings using GloVe (Pennington et al., 2014) and Continuous Skipgram (Mikolov, Chen, et al., 2013) with negative sampling (SGNS).
4. reported results
Accuracy for 2 models are reported on Simlex-999 (Hill et al., 2015) and the Google Analogies Dataset (Mikolov, Chen, et al., 2013)
5. paper's conclusions and/or main contributions
Created the repository and web servcie with pre-trained vectors for English and Norwegian.  ",No,marginal relevance,,,Short,Demo paper
"@inproceedings{dakota-kubler-2017-towards,
    title = ""Towards Replicability in Parsing"",
    author = {Dakota, Daniel  and
      K{\""u}bler, Sandra},
    booktitle = ""Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://doi.org/10.26615/978-954-452-049-6_026"",
    doi = ""10.26615/978-954-452-049-6_026"",
    pages = ""185--194"",
    abstract = ""We investigate parsing replicability across 7  languages  (and  8  treebanks),  showing that choices concerning the use of grammatical functions in parsing or evaluation and the influence of the rare word threshold,  as  well  as  choices  in  test  sentences and  evaluation  script  options  have  considerable and often unexpected effects on parsing  accuracies.   All  of  those  choices need  to  be  carefully  documented  if  we want to ensure replicability.""
}
","Dakota, Daniel  and
      K{\""u}bler, Sandra},
    booktitle = ""Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP",Towards Replicability in Parsing,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",2017,WS,"We investigate parsing replicability across 7  languages  (and  8  treebanks),  showing that choices concerning the use of grammatical functions in parsing or evaluation and the influence of the rare word threshold,  as  well  as  choices  in  test  sentences and  evaluation  script  options  have  considerable and often unexpected effects on parsing  accuracies.   All  of  those  choices need  to  be  carefully  documented  if  we want to ensure replicability.","1. one sentence summary of what paper is about
Paper diagnoses wide-spread problem in parsing research of not documenting all aspects of system implementation and evaluation, and demonstrates the range of results obtained with different implementational and evaluation choices across 7 languages and 8 treebanks.
2. paper's central goal/question/hypothesis
Sets out to explore just how big a difference implementation and evaluation choices can make to parsing results.
3. methodology
Using the state-of-the-art Berkeley parser, authors train and test it on 8 treebanks with systematically varied choices on multiple dimensions including types of preprocessing and evaluation script parameters.
4. reported results
Different choices produce improvement of around 10 F1 score points on average from removal of grammatical functions, slight worsening from higher rare-words thresholds, and up to 2.3 F score points worse from test set reduction (higher for higher reduction factors).
5. paper's conclusions and/or main contributions
Many settings, parameters and pre/post-processing components have substantial effects on results, and should therefore be meticulously reported and documented in order to enable reproduction",,"* documents the lack of information about important (and score-affecting) details such as treebank preprocessing and evaluation scripts, even in leading papers
* main point in paper is lack of documentation making replciation difficult, but this paper itself is less than perfect (e.g. no links to scripts)
* paper doesn't conduct reproduction attempts, but takes and existing parser and systematically explores the effect of different script settings and component choices on results across multiple languages and datasets.",,,,
"@article{dror-etal-2017-replicability,
    title = ""Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets"",
    author = ""Dror, Rotem  and
      Baumer, Gili  and
      Bogomolov, Marina  and
      Reichart, Roi"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""5"",
    year = ""2017"",
    url = ""https://www.aclweb.org/anthology/Q17-1033"",
    doi = ""10.1162/tacl_a_00074"",
    pages = ""471--486"",
    abstract = ""With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.""
}
","Dror, Rotem  and
      Baumer, Gili  and
      Bogomolov, Marina  and
      Reichart, Roi",Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,Transactions of the Association for Computational Linguistics,2017,TACL,"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.","1. one sentence summary of what paper is about
[text]
2. paper's central goal/question/hypothesis
[text]
3. methodology
[text]
4. reported results
[text]
5. paper's conclusions and/or main contributions
[text]
",,,,,,
"@inproceedings{marrese-taylor-matsuo-2017-replication,
    title = ""Replication issues in syntax-based aspect extraction for opinion mining"",
    author = ""Marrese-Taylor, Edison  and
      Matsuo, Yutaka"",
    booktitle = ""Proceedings of the Student Research Workshop at the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics"",
    month = apr,
    year = ""2017"",
    address = ""Valencia, Spain"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/E17-4003"",
    pages = ""23--32"",
    abstract = ""Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.""
}
","Marrese-Taylor, Edison  and
      Matsuo, Yutaka",Replication issues in syntax-based aspect extraction for opinion mining,Proceedings of the Student Research Workshop at the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,2017,EACL,"Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.","1. one sentence summary of what paper is about
[text]
2. paper's central goal/question/hypothesis
[text]
3. methodology
[text]
4. reported results
[text]
5. paper's conclusions and/or main contributions
[text]
",,,,,,
"@inproceedings{horsmann-zesch-2017-lstms,
    title = ""Do {LSTM}s really work so well for {P}o{S} tagging? {--} A replication study"",
    author = ""Horsmann, Tobias  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"",
    month = sep,
    year = ""2017"",
    address = ""Copenhagen, Denmark"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/D17-1076"",
    doi = ""10.18653/v1/D17-1076"",
    pages = ""727--736"",
    abstract = ""A recent study by Plank et al. (2016) found that  LSTM-based  PoS  taggers  considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset.   We  replicate  this study  using  a  fresh  collection  of  27  corpora  of  21  languages  that  are  annotated with fine-grained tagsets of varying size. Our replication confirms the result in general, and we additionally find that the advantage of LSTMs is even bigger for larger tagsets.   However,  we  also  find  that  for the very large tagsets of morphologically rich  languages,  hand-crafted  morphological  lexicons  are  still  necessary  to  reach state-of-the-art performance.""
}
","Horsmann, Tobias  and
      Zesch, Torsten",Do {LSTM}s really work so well for {P}o{S} tagging? {--} A replication study,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017,EMNLP,"A recent study by Plank et al. (2016) found that  LSTM-based  PoS  taggers  considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset.   We  replicate  this study  using  a  fresh  collection  of  27  corpora  of  21  languages  that  are  annotated with fine-grained tagsets of varying size. Our replication confirms the result in general, and we additionally find that the advantage of LSTMs is even bigger for larger tagsets.   However,  we  also  find  that  for the very large tagsets of morphologically rich  languages,  hand-crafted  morphological  lexicons  are  still  necessary  to  reach state-of-the-art performance.","1. one sentence summary of what paper is about
Paper picks up strong results by Plank et al. (2016) for LSTM tagging on coarse-grained tag sets, and investigates whether they can be confirmed for finer-grained tag sets.
2. paper's central goal/question/hypothesis
To replicate Plank et al.'s results that LSTM taggers are better than comparable CRF and HMM taggers, for fine-grained tag sets, looking also at impact of tag set size.
3. methodology
Using a fresh set of 27 corpora (21 languages) with fine-grained tag sets, Plank's LSTM, a self-implemented CRF and an off-the-shelf HMM tagger, systematically compare results for the 3 models on the 27 corpora
4. reported results
LSTM generally better, but with smaller tag set sizes differences between LSTM, CRF and HMM taggers are often small. Relative superiority of LSTM tagger grows in proportion to tag set size.
5. paper's conclusions and/or main contributions
The study confirmed that the LSTM tagger reported by Plank et al. (2016) also outperforms CRF and HMM taggers on fine-grained tagsets. LSTM tagger also comes closest to reaching performance levels of language-specific taggers from the literature, outperforming the latter by small margins on two out of three languages on which this was tested according to Table 4; very thin evidence for concluding ""[o]n morphologically fine tagsets, even the LSTM tagger fails to reach results reported in the literature when reproducing those setups"" - one single result on very fine grained tag set.
",,"* this aims to reproduce a general result, namely that LSTMs are better than CRFs at language-independent POS-tagger training
* doesn't distinguish between 'tagger' and 'method for creating a tagger' which isn't helpful
* this seems to be an adversarial reproduction attempt, i.e. they were expecting results not to be better for LSTM when switching to fine grained tagsets",,,,
"@inproceedings{morey-etal-2017-much,
    title = ""How much progress have we made on {RST} discourse parsing? A replication study of recent results on the {RST}-{DT}"",
    author = ""Morey, Mathieu  and
      Muller, Philippe  and
      Asher, Nicholas"",
    booktitle = ""Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"",
    month = sep,
    year = ""2017"",
    address = ""Copenhagen, Denmark"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/D17-1136"",
    doi = ""10.18653/v1/D17-1136"",
    pages = ""1319--1324"",
    abstract = ""This article evaluates purported progress over the past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51% on all metrics that authors attribute to the introduction of distributed representations of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to distributed representations represent at most a 16% relative error reduction on fully-labelled structures.""
}
","Morey, Mathieu  and
      Muller, Philippe  and
      Asher, Nicholas",How much progress have we made on {RST} discourse parsing? A replication study of recent results on the {RST}-{DT},Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017,EMNLP,"This article evaluates purported progress over the past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51% on all metrics that authors attribute to the introduction of distributed representations of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to distributed representations represent at most a 16% relative error reduction on fully-labelled structures.","1. one sentence summary of what paper is about
This paper replicates the evaluation of the parsers on the test set of Rhetorical Structure Theory Discourse Treebank (RST-DT) dataset and argues to re-examine and standardize the evaluation procedures used for RST discourse parsing
2. paper's central goal/question/hypothesis
Evaluate the progress in RST discoure plannig. Does distributed representations of discourse units really help in error reduction?
3. methodology
Reproduce predictions for 9 parsers from 8 studies. Identified two groups of studies - one which reports micro F1 scores and the other reports macro F1.
Used standard Parseval procedure instead of Marcu’s adaptation RST-Parseval as the main evaluation procedure and compute scores on discourse structures with no label (S for for Span or labelled with nuclearity (N), relation (R) or both (F for Full)). 
4. reported results
Replication attempts of micro-averaged F1 scores for one group and macro-averaged F1 for the other are reported. Finally Micro-averaged F1 scores using original Parseval are provided for future benchmark
5. paper's conclusions and/or main contributions
Increase in parser perfomance is due to the different implementations of evaluation procedures. Calls for 
",No,*Makes an argument about the implementation of evaluation procedure as well as the averaging method for reporting results (micro vs macro F1),,,,
"@inproceedings{htut-etal-2018-grammar,
    title = ""Grammar Induction with Neural Language Models: An Unusual Replication"",
    author = ""Htut, Phu Mon  and
      Cho, Kyunghyun  and
      Bowman, Samuel"",
    booktitle = ""Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}"",
    month = nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W18-5452"",
    doi = ""10.18653/v1/W18-5452"",
    pages = ""371--373"",
    abstract = ""Grammar induction is the task of learning syntactic structure without the expert-labeled treebanks (Charniak and Carroll, 1992; Klein and Manning, 2002). Recent work on latent tree learning offers a new family of approaches to this problem by inducing syntactic structure using the supervision from a downstream NLP task (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). In a recent paper published at ICLR, Shen et al. (2018) introduce such a model and report near state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. During the analysis of this model, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we analyze the model under different configurations to understand what it learns and to identify the conditions under which it succeeds. We find that this model represents the first empirical success for neural network latent tree learning, and that neural language modeling warrants further study as a setting for grammar induction.""
}
","Htut, Phu Mon  and
      Cho, Kyunghyun  and
      Bowman, Samuel",Grammar Induction with Neural Language Models: An Unusual Replication,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},2018,WS,"Grammar induction is the task of learning syntactic structure without the expert-labeled treebanks (Charniak and Carroll, 1992; Klein and Manning, 2002). Recent work on latent tree learning offers a new family of approaches to this problem by inducing syntactic structure using the supervision from a downstream NLP task (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). In a recent paper published at ICLR, Shen et al. (2018) introduce such a model and report near state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. During the analysis of this model, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we analyze the model under different configurations to understand what it learns and to identify the conditions under which it succeeds. We find that this model represents the first empirical success for neural network latent tree learning, and that neural language modeling warrants further study as a setting for grammar induction.","1. one sentence summary of what paper is about
Recreates an existing grammar induction model (PRPN, Shen et al., 2018) and tests different versions of it with different datasets and different model configurations, some of which perform very well.
2. paper's central goal/question/hypothesis
To explore and analyse the PRPN model and its performance under different conditions.
3. methodology
Recreated PRPN model in different configurations (similarity of any one of them to original model(s) is unclear) and trained in on different data sets (unclear how similar any of these are to those used in original paper) and calculated F1 on LB, RB, unlabelled parsing accuracy.
4. reported results
Reports results competitive with supervised WSJ parsing from early 2000s. Tuning on parsing task rather than LM task is not helpful.
5. paper's conclusions and/or main contributions
PRPN model ""strikingly effective"" at latent tree learning. Joint modelling of parsing and LM seems to work best.
",,"* very hard to follow, far too short, no proper explanation of how replicated and how went beyond
* mentions replicating perpexity score of one model, but original paper doesn't have the same value (61.1) in it
* all we really know is that they recreated a model in various versions, but not what they tried to recreate exactly, under what conditions, and what the outcome was in each case",,,,
"@article{crane-2018-questionable,
    title = ""Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results"",
    author = ""Crane, Matt"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""6"",
    year = ""2018"",
    url = ""https://www.aclweb.org/anthology/Q18-1018"",
    doi = ""10.1162/tacl_a_00018"",
    pages = ""241--252"",
   abstract = ""Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field” (Pfeiffer and Hoffmann, 2009). As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.""
}
","Crane, Matt",Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results,Transactions of the Association for Computational Linguistics,2018,TACL,"Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field” (Pfeiffer and Hoffmann, 2009). As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.","1. one sentence summary of what paper is about
Using answer selection in QA (TrecQA and WikiQA dataset) as the underlying task, this paper demonstrates number of factors (hardware / software / environment) that affects results of Deep Learning models. 
2. paper's central goal/question/hypothesis
Distribution of source code is not enough for reproducibility unless the running environment is reported
3. methodology
Used PyTorch re-implementation Sequiera et al. (2017) of Severyn and Moschitti (2015) model and varied different configurations of hardware and software on both the datasets like the GPU configuration, seed, number of threads, framework used, etc. 
4. reported results
Average Precision (AP) and Reciprocal Rank (RR) are reported for differnet runs of the same model in different configurations of the environment
5. paper's conclusions and/or main contributions
Largest source of variability - when the network was seeded with different random starting points.
changing the hardware, number of threads and the math library can also change the results
model definition, library versions and dependencies should be specified either through Docker or provide pre-trained model
","* Mentions issues in IR of weak baselines (Armstrong et al., 2009) and reproducibility (Arguello et al., 2016; Lin et al., 2016)","Nice references such as : 
* need for replicable and reproducible results being included in a list of challenges for the ACL Challenges for ACL: ACL Presidential Address 2017.https://www.slideshare.net/aclanthology/ joakim-nivre-2017-presidential- address-acl-2017-challenges-for- acl/ 
*Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field” (Pfeiffer and Hoffmann, 2009).  -- can use this in journal paper
* Simon (2017) observed a 16% increase of test accuracy for the same model (from 0.5438 to 0.6197) by changing the computation backend from Tensorflow to MXNet.",,,,
"@inproceedings{branco-2018-depleting,
    title = ""We Are Depleting Our Research Subject as We Are Investigating It: In Language Technology, more Replication and Diversity Are Needed"",
    author = ""Branco, Ant{\'o}nio"",
    booktitle = ""Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)"",
    month = may,
    year = ""2018"",
    address = ""Miyazaki, Japan"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://www.aclweb.org/anthology/L18-1022"",
    abstract = ""In this paper, we present an analysis indicating that, in language technology, as we are investigating natural language we are contributing to deplete it in the sense that we are contributing to reduce the diversity of languages.  To address this circumstance, we propose that more replication and reproduction and more language diversity need to be taken into account in our research activities.""
}
","Branco, Ant{\'o}nio","We Are Depleting Our Research Subject as We Are Investigating It: In Language Technology, more Replication and Diversity Are Needed",Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),2018,LREC,"In this paper, we present an analysis indicating that, in language technology, as we are investigating natural language we are contributing to deplete it in the sense that we are contributing to reduce the diversity of languages.  To address this circumstance, we propose that more replication and reproduction and more language diversity need to be taken into account in our research activities.","1. one sentence summary of what paper is about
Position paper/opinion piece which argues that the NLP community should focus on language diveristy as part of science ethics. On a similar note, the paper also advocates reproducibility and replicability of research results.  
2. paper's central goal/question/hypothesis
To provide analysis of the depleting role of different languages in the NLP community
3. methodology
Not a replication study but an opinion piece. Provides analysis and literature review backing the claims of depleting diversity from the technological viewpoint. 
4. reported results
No results are reported
5. paper's conclusions and/or main contributions
Provides analysis and claims depleting language diversity is two fold - 1. NLP researchers only focus on English 2. Historical technological shock providing social and economical competitive advantages to the English language
","Following the text introducing a new Special Section of the Language Re- sources and Evaluation journal on reproducibility and repli- cability (Branco et al., 2017): “Reproduction of results en- tails arriving at the same overall conclusion(s), as opposed to finding identical values for some measure (Drummond, 2009), (Dalle, 2012), (Buchert and Nussbaum, 2011); that is, to appropriately validate a set of results, scientists should strive to reproduce the same answer to a given research question by different means, possibly by re-implementing an algorithm or evaluating it on a new dataset. Replication has a somewhat more limited aim, typically involving run- ning the exact same system under the same conditions in order to arrive at the same output result.”",marginal,NA,,Short,
"@inproceedings{cohen-etal-2018-three,
    title = ""Three Dimensions of Reproducibility in Natural Language Processing"",
    author = ""Cohen, K. Bretonnel  and
      Xia, Jingbo  and
      Zweigenbaum, Pierre  and
      Callahan, Tiffany  and
      Hargraves, Orin  and
      Goss, Foster  and
      Ide, Nancy  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Grouin, Cyril  and
      Hunter, Lawrence E."",
    booktitle = ""Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)"",
    month = may,
    year = ""2018"",
    address = ""Miyazaki, Japan"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://www.aclweb.org/anthology/L18-1025"",
   abstract = ""Despite  considerable  recent  attention  to  problems  with  reproducibility  of  scientific  research,  there  is  a  striking  lack  of  agreement about the definition of the term.  That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing.  This paper proposes anontology of reproducibility in that field. Its goal is to enhance both future research and communication about the topic, and retrospective meta-analyses. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of aconclusion, of a finding, and ofavalue. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions.""
}
","Cohen, K. Bretonnel  and
      Xia, Jingbo  and
      Zweigenbaum, Pierre  and
      Callahan, Tiffany  and
      Hargraves, Orin  and
      Goss, Foster  and
      Ide, Nancy  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Grouin, Cyril  and
      Hunter, Lawrence E.",Three Dimensions of Reproducibility in Natural Language Processing,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),2018,LREC,"Despite  considerable  recent  attention  to  problems  with  reproducibility  of  scientific  research,  there  is  a  striking  lack  of  agreement about the definition of the term.  That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing.  This paper proposes anontology of reproducibility in that field. Its goal is to enhance both future research and communication about the topic, and retrospective meta-analyses. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of aconclusion, of a finding, and ofavalue. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions.","1. one sentence summary of what paper is about
[text]
2. paper's central goal/question/hypothesis
[text]
3. methodology
[text]
4. reported results
[text]
5. paper's conclusions and/or main contributions
[text]
",,,,,,
"@inproceedings{gartner-etal-2018-preserving,
    title = ""Preserving Workflow Reproducibility: The {R}e{P}lay-{DH} Client as a Tool for Process Documentation"",
    author = {G{\""a}rtner, Markus  and
      Hahn, Uli  and
      Hermann, Sibylle},
    booktitle = ""Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)"",
    month = may,
    year = ""2018"",
    address = ""Miyazaki, Japan"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://www.aclweb.org/anthology/L18-1089"",
   abstract = ""In  this  paper  we  present  a  software  tool  for  elicitation  and  management  of  process  metadata.   It  follows  our  previously  published design idea of an assistant for researchers that aims at minimizing the additional effort required for producing a sustainable workflow documentation.   With  the  ever-growing  number  of  linguistic  resources  available,  it  also  becomes  increasingly  important  to  provide proper  documentation  to  make  them  comparable  and  to  allow  meaningful  evaluations  for  specific  use  cases.   The  often  prevailing practice  of  post  hoc  documentation  of  resource  generation  or  research  processes  bears  the  risk  of  information  loss.   Not  only  does detailed documentation of a process aid in achieving reproducibility, it also increases usefulness of the documented work for others asa cornerstone of good scientific practice.  Time pressure together with the lack of simple documentation methods leads to workflow documentation in practice being an arduous and often neglected task. Our tool ensures a clean documentation for common workflows innatural language processing and digital humanities. Additionally, it can easily be integrated into existing institutional infrastructures."",
}
","G{\""a}rtner, Markus  and
      Hahn, Uli  and
      Hermann, Sibylle",Preserving Workflow Reproducibility: The {R}e{P}lay-{DH} Client as a Tool for Process Documentation,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),2018,LREC,"In  this  paper  we  present  a  software  tool  for  elicitation  and  management  of  process  metadata.   It  follows  our  previously  published design idea of an assistant for researchers that aims at minimizing the additional effort required for producing a sustainable workflow documentation.   With  the  ever-growing  number  of  linguistic  resources  available,  it  also  becomes  increasingly  important  to  provide proper  documentation  to  make  them  comparable  and  to  allow  meaningful  evaluations  for  specific  use  cases.   The  often  prevailing practice  of  post  hoc  documentation  of  resource  generation  or  research  processes  bears  the  risk  of  information  loss.   Not  only  does detailed documentation of a process aid in achieving reproducibility, it also increases usefulness of the documented work for others asa cornerstone of good scientific practice.  Time pressure together with the lack of simple documentation methods leads to workflow documentation in practice being an arduous and often neglected task. Our tool ensures a clean documentation for common workflows innatural language processing and digital humanities. Additionally, it can easily be integrated into existing institutional infrastructures.","1. one sentence summary of what paper is about
Demo paper describing the system build for workflow management.
2. paper's central goal/question/hypothesis
Build a workflow management tool; supporting process documentation by using version control as foundation
3. methodology
Implemented the tool in Java, open-sourced for reproducible research.
4. reported results
NA
5. paper's conclusions and/or main contributions
""RePlay-DH"" system description, motivating its use case from the point of reporoducibility
",Claerbout and Karrenbach (1992) ,marginal relevance,,,,
"@inproceedings{horsmann-zesch-2018-deeptc,
    title = ""{D}eep{TC} {--} An Extension of {DKP}ro Text Classification for Fostering Reproducibility of Deep Learning Experiments"",
    author = ""Horsmann, Tobias  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)"",
    month = may,
    year = ""2018"",
    address = ""Miyazaki, Japan"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://www.aclweb.org/anthology/L18-1403"",
    abstract = ""We present a deep learning extension for the multi-purpose text classification framework DKPro Text Classification (DKPro TC). DKProTC is a flexible framework for creating easily shareable and reproducible end-to-end NLP experiments involving machine learning. We provide an overview of the current state of DKPro TC, which does not allow integration of deep learning, and discuss the necessary conceptual extensions.  These extensions are based on an analysis of common deep learning setups found in the literature to support all common text classification setups, i.e. single outcome, multi outcome, and sequence classification problems.  Additionally to providing an end-to-end shareable environment for deep learning experiments, we provide convenience features that take care of repetitive steps, such as pre-processing, data vectorization and pruning of embeddings.  By moving a large part of this boilerplate code into DKPro TC, the actual deep learning framework code improves in readability and lowers the amount of redundant source code considerably.  As proof-of-concept, we integrate Keras, DyNet, and DeepLearning4J."",
}
","Horsmann, Tobias  and
      Zesch, Torsten",{D}eep{TC} {--} An Extension of {DKP}ro Text Classification for Fostering Reproducibility of Deep Learning Experiments,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),2018,LREC,"We present a deep learning extension for the multi-purpose text classification framework DKPro Text Classification (DKPro TC). DKProTC is a flexible framework for creating easily shareable and reproducible end-to-end NLP experiments involving machine learning. We provide an overview of the current state of DKPro TC, which does not allow integration of deep learning, and discuss the necessary conceptual extensions.  These extensions are based on an analysis of common deep learning setups found in the literature to support all common text classification setups, i.e. single outcome, multi outcome, and sequence classification problems.  Additionally to providing an end-to-end shareable environment for deep learning experiments, we provide convenience features that take care of repetitive steps, such as pre-processing, data vectorization and pruning of embeddings.  By moving a large part of this boilerplate code into DKPro TC, the actual deep learning framework code improves in readability and lowers the amount of redundant source code considerably.  As proof-of-concept, we integrate Keras, DyNet, and DeepLearning4J.","1. one sentence summary of what paper is about
Demo paper describing DeepTC an extension of previous DKProTC framework for creating sharable and reproducible text classification models.
2. paper's central goal/question/hypothesis
Provide overview of the DL extension and integrate three libraries (Keras, DyNet and Deeplearning 4J) 
3. methodology
Implemented plain LSTM in three frameworks (Keras, DyNet and Deeplearning 4J) and experiment on WSJ for PoS tagging. Compared with shallow CRF from DKProTC
4. reported results
Accuracy on WSJ sections using reproduction is reported. However the original results claimed by the different frameworks is not provided. 
5. paper's conclusions and/or main contributions
Open-source system with DL extension. Showed that replication study allows to replicate state-of-the-art result but didnt compare with the original. 
",None,"* frequent challenges to re- production of deep learning experiment incomplete data preparation steps, embedding preparations tasks, and the vectorization of data into an integer representation.
* Didnt compare the reproduced results with original. However makes the following important points regarding DL 
The rapid development of deep learning software creates practical limitations to reproducibility and convenience
Many deep learning frameworks are still under rapid development and, thus, change quickly with bugs being fixed and APIs being updated.",,,,
"@article{wieling-etal-2018-squib,
    title = ""{S}quib: Reproducibility in Computational Linguistics: Are We Willing to Share?"",
    author = ""Wieling, Martijn  and
      Rawee, Josine  and
      van Noord, Gertjan"",
    journal = ""Computational Linguistics"",
    volume = ""44"",
    number = ""4"",
    month = dec,
    year = ""2018"",
    url = ""https://www.aclweb.org/anthology/J18-4003"",
    doi = ""10.1162/coli_a_00330"",
    pages = ""641--649"",
}
","Wieling, Martijn  and
      Rawee, Josine  and
      van Noord, Gertjan",{S}quib: Reproducibility in Computational Linguistics: Are We Willing to Share?,Computational Linguistics,2018,CL,"This study focuses on an essential precondition for reproducibility in computational linguistics: the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen’s influential “Last Words” contribution in Computational Linguistics, we investigate to what extent researchers in computational linguistics are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the code in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, empiricism in computational linguistics still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers: The median citation count for studies with working links to the source code is higher.","1. one sentence summary of what paper is about
Looks at availability of code and data in NLP papers, and reproducibility of results based on them.
2. paper's central goal/question/hypothesis
Investigates how often data and source code are available or can be obtained for two years of ACL papers, and attempts reproduction for 10 papers for which data/code were available/obtainable. Tests whether papers with linked code/data is cited more.
3. methodology
Random selection (except 1 from 2016) of 5 papers each from sets of all full papers using some code and some data from ACL 2011 and 2016, for which said code and data could be obtained. Authors contacted for help if needed. Max 8 hours work by Masters student for each attempt.
4. reported results
Results provided for each of the 10 papers selected for reproduction. Unclear how scores were selected for reproduction (not all are attempted), reasons for failure not always clear even from linked material.  
5. paper's conclusions and/or main contributions
Obtainability of code and data substantially improved from 2011 to 2016, more dramatically for code. Higher citation counts for papers with linked data and code.
","``In line with Liberman (2015) and Barba (2018), we reject the unfortunate swap in the meaning of reproduction and replication by Drummond (2009). Consequently, with reproduction (or reproducibility), we denote the exact re-creation of the results reported in a publication using the same data and methods.'' ","* the methodology used is not very rigorous, e.g. MSc student doing the reproduction, random 8h cut-off, unexplained selection of scorres for reproduction
* other papers e.g. Mieskes et al. (2019) over-interpret this study e.g. claiming only a single reproduction obtained the same results and 60% of attempts failed. ",,,,
"@inproceedings{htut-etal-2018-grammar-induction,
    title = ""Grammar Induction with Neural Language Models: An Unusual Replication"",
    author = ""Htut, Phu Mon  and
      Cho, Kyunghyun  and
      Bowman, Samuel"",
    booktitle = ""Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"",
    month = oct # ""-"" # nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/D18-1544"",
    doi = ""10.18653/v1/D18-1544"",
    pages = ""4998--5003"",
}
","Htut, Phu Mon  and
      Cho, Kyunghyun  and
      Bowman, Samuel",Grammar Induction with Neural Language Models: An Unusual Replication,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,2018,EMNLP,"A substantial thread of recent work on latent
tree learning has attempted to develop neural
network models with parse-valued latent variables and train them on non-parsing tasks, in
the hope of having them discover interpretable
tree structure. In a recent paper, Shen et al.
(2018) introduce such a model and report nearstate-of-the-art results on the target task of language modeling, and the first strong latent tree
learning result on constituency parsing. In an
attempt to reproduce these results, we discover
issues that make the original results hard to
trust, including tuning and even training on
what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets.
We find that the results of this work are robust:
All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar
induction systems. We find that this model
represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a
setting for grammar induction.","
1. one sentence summary of what paper is about
Replicate the PRPN model (Shen et al., 2018) learns and under what conditions for language modeling and grammar induction
2. paper's central goal/question/hypothesis
Is the PRPN model (Shen et al., 2018) robust? 
3. methodology
Used author's available code; did not re-implement or re-tune PRPN. However, they do not report reults of the replication, if they were exactly reproducible or not.  
4. reported results
Parsing F1 results evaluated on full WSJ10 and WSJ while perplexity on the WSJ test set for language modeling. Did not compare with the Shen et al. 
5. paper's conclusions and/or main contributions
Find several experimental design problems that make the results difficult to interpret; however the model is found to be robust
",No,"AB: is this the same paper as htut-etal-2018-grammar ? urls are different, but title and authors are the same. 
SA: that paper seems like an abstract while this is a short paper

** doesnt seem like a replication study",,https://www.aclweb.org/anthology/D18-1544,,
"@inproceedings{moore-rayson-2018-bringing,
    title = ""Bringing replication and reproduction together with generalisability in {NLP}: Three reproduction studies for Target Dependent Sentiment Analysis"",
    author = ""Moore, Andrew  and
      Rayson, Paul"",
    booktitle = ""Proceedings of the 27th International Conference on Computational Linguistics"",
    month = aug,
    year = ""2018"",
    address = ""Santa Fe, New Mexico, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/C18-1097"",
    pages = ""1132--1144"",
}
","Moore, Andrew  and
      Rayson, Paul",Bringing replication and reproduction together with generalisability in {NLP}: Three reproduction studies for Target Dependent Sentiment Analysis,Proceedings of the 27th International Conference on Computational Linguistics,2018,COLING,"Lack of repeatability and generalisability are two significant threats to continuing scientific development in Natural Language Processing. Language models and learning methods are so complex that scientific conference papers no longer contain enough space for the technical depth required
for replication or reproduction. Taking Target Dependent Sentiment Analysis as a case study, we
show how recent work in the field has not consistently released code, or described settings for
learning methods in enough detail, and lacks comparability and generalisability in train, test or
validation data. To investigate generalisability and to enable state of the art comparative evaluations, we carry out the first reproduction studies of three groups of complementary methods and
perform the first large-scale mass evaluation on six different English datasets. Reflecting on our
experiences, we recommend that future replication or reproduction experiments should always
consider a variety of datasets alongside documenting and releasing their methods and published
code in order to minimise the barriers to both repeatability and generalisability. We have released our code with a model zoo on GitHub with Jupyter Notebooks to aid understanding and full documentation, and we recommend that others do the same with their papers at submission time through an anonymised GitHub account.","1. one sentence summary of what paper is about
Investigated generalisability of the SOTA models from 3 papers for Target Dependent Sentiment Analysis and provided a mass evaluation study on 6 other english datasets
2. paper's central goal/question/hypothesis
Reproduced results of Vo and Zhang (2015),  Wang et al. (2017) and Tang et al. (2016a) on Dong et al. Twitter dataset
3. methodology
4 models from Vo and Zhang, 3 from Wang et al and Tang et al. respectively were reproduced. They were extended for mass evaluation on 6 other datasets for which different splits have been created and further open sourced for reproduciblity 
4. reported results
Accuracy is compared for the reproduced results with Vo and Zhang and Wang et al. respectively while macro F1 score is compared for Tang et al. 
5. paper's conclusions and/or main contributions
Able to reproduce results of three papers. Provided mass evaluation of the approaches on six different English datasets
","Followed definitions in Antske Fokkens’ guest blog post replication (obtaining the same results using the same
experiment) as well as reproduction (reach the same conclusion through different means).  http://coling2018. org/slowly-growing-offspring-zigglebottom-anno-2017-guest-post/","SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code.  Louridas and Gousios (2012) provided requirements in Software Engineering for reproducibility - (a) All data related to the paper, (b) All code required to reproduce the paper and (c) Documentation for the code and data. Fokkens et al. (2013) in WordNet similarity and Named Entity Recognition provided 5 key aspects (a) pre-processing, (b) experimental setup, (c) versioning, (d) system output, (e) system variation.   Makes a point about incomplete descriptions of methods and settings, patchy release of code, and lack of comparative evaluations in previous research",Yes,,,
"@inproceedings{pierrejean-tanguy-2018-etude,
    title = ""Etude de la reproductibilit{\'e} des word embeddings : rep{\'e}rage des zones stables et instables dans le lexique (Reproducibility of word embeddings : identifying stable and unstable zones in the semantic space)"",
    author = ""Pierrejean, B{\'e}n{\'e}dicte  and
      Tanguy, Ludovic"",
    booktitle = ""Actes de la Conf{\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN"",
    month = ""5"",
    year = ""2018"",
    address = ""Rennes, France"",
    publisher = ""ATALA"",
    url = ""https://www.aclweb.org/anthology/2018.jeptalnrecital-long.3"",
    pages = ""33--46"",
    language = ""French"",
}
","Pierrejean, B{\'e}n{\'e}dicte  and
      Tanguy, Ludovic",Etude de la reproductibilit{\'e} des word embeddings : rep{\'e}rage des zones stables et instables dans le lexique (Reproducibility of word embeddings : identifying stable and unstable zones in the semantic space),"Actes de la Conf{\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",2018,WS,Distributional semantic models trained using neural networks techniques yield different modelseven when using the same parameters. We describe a series of experiments where we examine theinstability of word embeddings both from a global and local perspective for several models trainedwith the same parameters. We measured the global variation for models trained on three differentcorpora. This variation is estimated to about 17% for the 25 nearest neighbours of a target word. Wealso identified and described local zones of stability and instability in the semantic space.,"1. one sentence summary of what paper is about
[text]
2. paper's central goal/question/hypothesis
[text]
3. methodology
[text]
4. reported results
[text]
5. paper's conclusions and/or main contributions
[text]
",,,,,,
"@inproceedings{van-miltenburg-etal-2019-task,
    title = ""On task effects in {NLG} corpus elicitation: a replication study using mixed effects modeling"",
    author = ""van Miltenburg, Emiel  and
      van de Kerkhof, Merel  and
      Koolen, Ruud  and
      Goudbeek, Martijn  and
      Krahmer, Emiel"",
    booktitle = ""Proceedings of the 12th International Conference on Natural Language Generation"",
    month = oct # ""{--}"" # nov,
    year = ""2019"",
    address = ""Tokyo, Japan"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W19-8649"",
    doi = ""10.18653/v1/W19-8649"",
    pages = ""403--408"",
}
","van Miltenburg, Emiel  and
      van de Kerkhof, Merel  and
      Koolen, Ruud  and
      Goudbeek, Martijn  and
      Krahmer, Emiel",On task effects in {NLG} corpus elicitation: a replication study using mixed effects modeling,Proceedings of the 12th International Conference on Natural Language Generation,2019,INLG,"Task effects in NLG corpus elicitation recentlystarted to receive more attention, but are usu-ally  not  modeled  statistically.   We  present  acontrolled replication of the study by Van Mil-tenburg et al. (2018b), contrasting spoken withwritten descriptions.   We collected additionalwritten Dutch descriptions to supplement thespoken data from the DIDEC corpus, and an-alyzed  the  descriptions  using  mixed  effectsmodeling to account for variation between par-ticipants and items.  Our results show that theeffects of modality largely disappear in a con-trolled setting","1. one sentence summary of what paper is about
Presents a controlled replication of the conclusion of Van Miltenburg et al. (2018b) which study modality effect in the elicitation of the NLG corpus data - spoken vs written descriptions.  
2. paper's central goal/question/hypothesis
Provide a controlled comparison between spoken and written image descriptions to isolate the effect of modality on the generated descriptions
3. methodology
Collected additional written Dutch descriptions to supplement the spoken data from the DIDEC corpus, and analyzed the descriptions using mixed effects modeling. Same images were used however a different sample from population was used to collect data.
4. reported results
Results are not compared for the values but instead on the findings and the conclusions. Statistics and p-values are reported for the difference spoken and written descriptions 
5. paper's conclusions and/or main contributions
Effects of modality largely disappear in a controlled setting - modality alone has a minimal effect on the content of the descriptions 
",,- Not exactly aiming for reproducibility but to measure the impact of controlled data collection on the replication study ,- Not exactly aiming for reproducibility but to measure the impact of controlled data collection on the replication study ,,,
"@inproceedings{fortuna-etal-2019-stop,
    title = ""Stop {P}ropag{H}ate at {S}em{E}val-2019 Tasks 5 and 6: Are abusive language classification results reproducible?"",
    author = ""Fortuna, Paula  and
      Soler-Company, Juan  and
      Nunes, S{\'e}rgio"",
    booktitle = ""Proceedings of the 13th International Workshop on Semantic Evaluation"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/S19-2131"",
    doi = ""10.18653/v1/S19-2131"",
    pages = ""745--752"",
}
","Fortuna, Paula  and
      Soler-Company, Juan  and
      Nunes, S{\'e}rgio",Stop {P}ropag{H}ate at {S}em{E}val-2019 Tasks 5 and 6: Are abusive language classification results reproducible?,Proceedings of the 13th International Workshop on Semantic Evaluation,2019,WS,"This paper summarizes the participation of Stop PropagHate team at SemEval 2019. Our approach is based on replicating one of the most relevant works on the literature, using word embeddings and LSTM. After circumventing some of the problems of the original code, we found poor results when applying it to the HatEval contest (F1=0.45). We think this is due mainly to inconsistencies in the data of this contest. Finally, for the OffensEval the classifier performed well (F1=0.74), proving to have a better performance for offense detection than for hate speech.","1. one sentence summary of what paper is about
paper describes authors' participation in HatEval and OffensEval 2019, but also their difficulties in recreating the high-performing and highly-cited work by Badjitya et al. (2017)
2. paper's central goal/question/hypothesis
Can Badjitya et al. (2017) be recreated and run with success at the two shared tasks?
3. methodology
Using code and related info provided by original authors, same data etc., recreate system; then test variations of it on shared tasks.
4. reported results
System could not be recreated because of missing information and serious bug detected in cross-validation where runs 2-10 started from system as learnt in preceding run rather than learning from scratch. A reimplemented, improved version did well at OffensEval, but badly at HatEval.
5. paper's conclusions and/or main contributions
full code sharing is only way to achieve reproducibility; results in Badjitya et al. are not reliable; similar, corrected system did well at one shared task, less well at the other which may be connected to inconsistencies between training and test data in the latter
","* this is quite an adversarial reproduction attempt
* they cite Lee et al (2018) as also having tried and failed to reproduce the Badjitya et al work but that paper doesn't mention such an attempt
* still, the rest of it seems robust and rigorous enough",marginal,,,,
"@inproceedings{mieskes-etal-2019-community,
    title = ""Community Perspective on Replicability in Natural Language Processing"",
    author = {Mieskes, Margot  and
      Fort, Kar{\""e}n  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Grouin, Cyril  and
      Cohen, Kevin},
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)"",
    month = sep,
    year = ""2019"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://www.aclweb.org/anthology/R19-1089"",
    doi = ""10.26615/978-954-452-056-4_089"",
    pages = ""768--775"",
}
","Mieskes, Margot  and
      Fort, Kar{\""e}n  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Grouin, Cyril  and
      Cohen, Kevin",Community Perspective on Replicability in Natural Language Processing,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),2019,WS,"With recent efforts in drawing attention to the task of replicating and/or reproducing results, for example in the context of COLING 2018 and various LREC workshops, the question arises how the NLP community views the topic of replicability in general. Using a survey, in which we involve members of the NLP community, we investigate how our community perceives this topic, its relevance and options for improvement. Based on over two hundred participants, the survey results confirm earlier observations, that successful reproducibility requires more than having access to code and data. Additionally, the results show that the topic has to be tackled from the authors’, reviewers’ and community’s side.","1. one sentence summary of what paper is about
Paper describes and reports results of a survey of 225 NLP researchs on topics in reproducibility
2. paper's central goal/question/hypothesis
by conducting survey of NLP researchers, gain understanding of factors that support or hinder awareness of reproducibility and of the role  each  individual  plays  as  an  author,  as  a  reviewer and as part of the NLP community
3. methodology
questionnaire of 18 questions in three categories:  (i) replication work in general, (ii) replicating one’s own work and (iii) replicating others’ work; distributed via LinkedIn and BioNLP, Corpora, LN and GLCL mailing lists.
4. reported results
headline results: 24.9% of attempts to reproduce own work, and 56.7% of attempts to reproduce another team's results failed to reach same conclusions; plus detailed survey results
5. paper's conclusions and/or main contributions
currently over half of reproduction attempts fail; maily because of unavailability of resources and documentaiton; authors should run reproduction attempts themselves before publication, then release the attempt for others to use; NLP community needs to develop guidelines; reproducibility-aware reviewing and community-developed guidelines would help 
","* is this representative: 38% PGR or Post-doc; 39% academic appointment; 22% other (=industry?)
* recruitment via mailling lists and LinkedIn - what was effect of self-selection?",,,,,
"@inproceedings{wu-etal-2019-errudite,
    title = ""{E}rrudite: Scalable, Reproducible, and Testable Error Analysis"",
    author = ""Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel"",
    booktitle = ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/P19-1073"",
    doi = ""10.18653/v1/P19-1073"",
    pages = ""747--763"",
}
","Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel","{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,2019,ACL,"Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.","1. one sentence summary of what paper is about
Demo paper which presents Errudite, an interactive tool for informative error analysis. 
2. paper's central goal/question/hypothesis
Characterize deficiencies with current error analysis methods used in NLP which can lead to high variance and low reproducibility
3. methodology
Error analysis of BiDAF on SQuAD using Errudite. Presents user study to study ambiguities in prior error analysis studies.
4. reported results
Mainly demo paper. Error coverage has been reported of replicating Seo et al. (Figure 7). High variance observed by user-defined groups in replication study.
5. paper's conclusions and/or main contributions
Motivated the use of the tool. Users unable to consistently replicate the analysis of Seo et al. (2016) due to the ambiguity inherent in manual grouping. ",None,* Not exactly reproducibility paper,,,Long,
"@article{zhang-duh-2020-reproducible,
    title = ""Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems"",
    author = ""Zhang, Xuan  and
      Duh, Kevin"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    url = ""https://www.aclweb.org/anthology/2020.tacl-1.26"",
    doi = ""10.1162/tacl_a_00322"",
    pages = ""393--408"",
}
","Zhang, Xuan  and
      Duh, Kevin",Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems,Transactions of the Association for Computational Linguistics,2020,ACL,"Hyperparameter selection is a crucial part of
building neural machine translation (NMT)
systems across both academia and industry.
Fine-grained adjustments to a model’s architecture or training recipe can mean the difference between a positive and negative research
result or between a state-of-the-art and underperforming system. While recent literature has
proposed methods for automatic hyperparameter optimization (HPO), there has been limited
work on applying these methods to neural
machine translation (NMT), due in part to the
high costs associated with experiments that
train large numbers of model variants. To
facilitate research in this space, we introduce
a lookup-based approach that uses a library
of pre-trained models for fast, low cost HPO
experimentation. Our contributions include (1)
the release of a large collection of trained NMT
models covering a wide range of hyperparameters, (2) the proposal of targeted metrics for
evaluating HPO methods on NMT, and (3) a
reproducible benchmark of several HPO methods against our model library, including novel
graph-based and multiobjective methods.","1. one sentence summary of what paper is about
This paper doesn't want to reproduce any previous research but instead want to enable reproducible HPO research on NMT tasks
2. paper's central goal/question/hypothesis
Create a benchmark dataset for hyperparameter optimization (HPO) of NMT models
3. methodology
Ran a large collection of NMT models with various hyperparameter configuration and recorded their performance. Adopted table-lookup procedure. Released this table dataset of all models with their configurations and perfomances. 
4. reported results
Used 3 evaluation metric fixed-target all (fta), fixed-target one (fto) and fixed-budget (fbp) based on measuring runtime and provided benchmark results.   
5. paper's conclusions and/or main contributions
3 fold - 1. Released large collection of trained NMT models 2. Proposed eval metrics for HPO 3. Reproducible benchmark / dataset",Followed two notions of reproducibility from Li and Talwalkar (2019): exact reproducibility (the reproducibility of reported experimental results); and broad reproducibility (the generalization of the experimental results),"Not trying reproducing any results but create a benchmark dataset. Argues that their benchmarks are exact reproducible (provide the tables that record all model results). Might not be broad reproducible, because the generalizability of the results might be restricted due to fixed collections of hyperparameter and variance associated with multiple runs",,,,
"@inproceedings{born-etal-2020-dataset,
    title = ""Dataset Reproducibility and {IR} Methods in Timeline Summarization"",
    author = ""Born, Leo  and
      Bacher, Maximilian  and
      Markert, Katja"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.218"",
    pages = ""1763--1771"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Born, Leo  and
      Bacher, Maximilian  and
      Markert, Katja",Dataset Reproducibility and {IR} Methods in Timeline Summarization,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems.","1. one sentence summary of what paper is about
The paper advocates for better development and evaluation of timeline summarisation systems: corpus collection should be done using explicit IR methods, rather than relying on external search engines.
2. paper's central goal/question/hypothesis
Do timeline summarisation systems (TLS) depend on preprocessing of the corpora they are trained on, namely IR methods and sentence filtering?
3. methodology
investigated the impact of three text-based IR methods and keyword-based sentence filters on TLS systems
4. reported results
A choice of an IR method impacts results of TLS. Also, performance of TLS systems is highly sensitive to sentence filtering.
5. paper's conclusions and/or main contributions
The study advocates for including IR into the development of TLS systems. That would make corpus collection for TLS reproducible.",no,"It is not a study on reproducibility per se.

Problem with TLS research: most of the corpora created for timeline summarisation are not publicly available; the corpora are collected via commercial search engines (this method is not reproducible and has biases).",,,,
"@inproceedings{antonio-rodrigues-etal-2020-reproduction,
    title = ""Reproduction and Revival of the Argument Reasoning Comprehension Task"",
    author = ""Ant{\'o}nio Rodrigues, Jo{\~a}o  and
      Branco, Ruben  and
      Silva, Jo{\~a}o  and
      Branco, Ant{\'o}nio"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.622"",
    pages = ""5055--5064"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Ant{\'o}nio Rodrigues, Jo{\~a}o  and
      Branco, Ruben  and
      Silva, Jo{\~a}o  and
      Branco, Ant{\'o}nio",Reproduction and Revival of the Argument Reasoning Comprehension Task,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"Reproduction of scientific findings is essential for scientific development across all scientific disciplines and reproducing results of previous works is a basic requirement for validating the hypothesis and conclusions put forward by them. This paper reports on the scientific reproduction of several systems addressing the Argument Reasoning Comprehension Task of SemEval2018. Given a recent publication that pointed out spurious statistical cues in the data set used in the shared task, and that produced a revised version of it, we also evaluated the reproduced systems with this new data set. The exercise reported here shows that, in general, the reproduction of these systems is successful with scores in line with those reported in SemEval2018. However, the performance scores are worst than those, and even below the random baseline, when the reproduced systems are run over the revised data set expunged from data artifacts. This demonstrates that this task is actually a much harder challenge than what could have been perceived from the inflated, close to human-level performance scores obtained with the data set used in SemEval2018. This calls for a revival of this task as there is much room for improvement until systems may come close to the upper bound provided by human performance.","1. one sentence summary of what paper is about
Reproduced 6 systems having participated at Argument Reasoning Comprehension Task of SemEval 2018. Also ran the systems on a revised version of the debiased dataset used in the task. 
2. paper's central goal/question/hypothesis
Reproduction of some systems addressing the argument reasoning comprehension task.
3. methodology
For reproduction, the paper followed system description and/or reused code if it was provided.
4. reported results
1 system score reproduced exactly, other 5 systems' scores reproduced within +- 0.036 delta; the initial ranking of SemEval was not affected. 
5. paper's conclusions and/or main contributions
Ran systems on the debiased dataset, showed that scores are lower, calls for a new edition of the task.
","following Stodden et al. (2014):

Replication, the practice of independently implementing scientific experiments to validate specific findings, is the cornerstone of discovering scientific truth. Related to replication is reproducibility, which is the calculation of quantitative scientific results by independent scientists using the original data sets and methods",,"the challenge ranking was reproduced. Some systems were harder to reproduce than others due to lack of instructions, library versioning, etc.

not part of REPROLANG",,,
"@inproceedings{branco-etal-2020-shared,
    title = ""A Shared Task of a New, Collaborative Type to Foster Reproducibility: A First Exercise in the Area of Language Science and Technology with {REPROLANG}2020"",
    author = ""Branco, Ant{\'o}nio  and
      Calzolari, Nicoletta  and
      Vossen, Piek  and
      Van Noord, Gertjan  and
      van Uytvanck, Dieter  and
      Silva, Jo{\~a}o  and
      Gomes, Lu{\'\i}s  and
      Moreira, Andr{\'e}  and
      Elbers, Willem"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.680"",
    pages = ""5539--5545"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
    abstract = ""In this paper, we introduce a new type of shared task — which is collaborative rather than competitive — designed to support and foster the reproduction of research results. We also describe the first event running such a novel challenge, present the results obtained, discuss the lessons learned and ponder on future undertakings."",
}
","Branco, Ant{\'o}nio  and
      Calzolari, Nicoletta  and
      Vossen, Piek  and
      Van Noord, Gertjan  and
      van Uytvanck, Dieter  and
      Silva, Jo{\~a}o  and
      Gomes, Lu{\'\i}s  and
      Moreira, Andr{\'e}  and
      Elbers, Willem","A Shared Task of a New, Collaborative Type to Foster Reproducibility: A First Exercise in the Area of Language Science and Technology with {REPROLANG}2020",Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"In this paper, we introduce a new type of shared task — which is collaborative rather than competitive — designed to support and foster the reproduction of research results. We also describe the first event running such a novel challenge, present the results obtained, discuss the lessons learned and ponder on future undertakings.","1. one sentence summary of what paper is about
Paper describes the design, organisation and results of the REPROLANG shared task at LREC
2. paper's central goal/question/hypothesis
To investigate designing a reproduction shared task and to produce results through the shared task which give insight about ability, difficulty and requirements for reproducing results from published papers across NLP
3. methodology
Shared task committee selected a final set of papers for reproduction via call for contribution and direct selection; very rigorous conditions were created for ensuring that reproduction attempts are themselves reproducible, including sharing all code via gitlab containers; submissions were reviewed by task organisers and original authors and scored on 4 reproduction scores
4. reported results
11 out of 18 papers were accepted, addressing 7 of the 11 papers up for reproduction. (No sucess/failure stats reported, or reviewing score stats.)
5. paper's conclusions and/or main contributions
reproductions generally possible; author collaboration helpful, but need to expand to reproducing any paper; shared task deemed a success.","* scoring of submissions confounds recreatability and reproducibility, as well as missing information and reproduction difficulties
* these were all collaborative reproduction attempts, so this is maybe not surprising: ""in general, the target papers pre-sented no special problems for their reproduction""
* one conclusion is that task may be more useful if reproduction of any paper is allowed rather than just those volunteered by their authors, but why not cite the Reproducibility Challenge in ML in this context?
* shame that submissions were not summarised - the real interest here would have been generalising over and drawing conclusions from all of the 11 reproduction outcomes",,,,,
"@inproceedings{garneau-etal-2020-robust,
    title = ""A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well"",
    author = ""Garneau, Nicolas  and
      Godbout, Mathieu  and
      Beauchemin, David  and
      Durand, Audrey  and
      Lamontagne, Luc"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.681"",
    pages = ""5546--5554"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Garneau, Nicolas  and
      Godbout, Mathieu  and
      Beauchemin, David  and
      Durand, Audrey  and
      Lamontagne, Luc",A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"In this paper, we reproduce the experiments of Artetxe et al. (2018b) regarding the robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. We show that the reproduction of their method is indeed feasible with some minor assumptions. We further investigate the robustness of their model by introducing four new languages that are less similar to English than the ones proposed by the original paper. In order to assess the stability of their model, we also conduct a grid search over sensible hyperparameters. We then propose key recommendations that apply to any research project in order to deliver fully reproducible research.","1. one sentence summary of what paper is about
Reproduction and robustness testing of the model for unsupervised cross-lingual word embeddings of Artexte et al.
2. paper's central goal/question/hypothesis
Are the experiments of Artetxe et al reproducible and robust (wrt hyperparameters and different languages)?
3. methodology
Used the provided codebase of Artetxe et al. Some missing parts (for ablation studies) were reimplemented. Also the robustness of the algorithm was tested by applying it to different languages and by varying some hyperparameters.
4. reported results
Main results from Artetxe et al were reproduced; their ablation studies were also reproduced with one experiment reproduction being unsuccessful. The algorithm performs less well on pairs of distant languages (not reported in the original study).
5. paper's conclusions and/or main contributions
The method of Artetxe et al is robust, when applied to the pairs of languages that share some commonalities. It seems to be not that robust with other pairs of languages (tested on EN-{ET, FA, LV, VI}). Recommendations for reproducibility (logging, dataset versionining, dockerisation)
",,,the main results --- yes,,,
"@inproceedings{khoe-2020-reproducing,
    title = ""Reproducing a Morphosyntactic Tagger with a Meta-{B}i{LSTM} Model over Context Sensitive Token Encodings"",
    author = ""Khoe, Yung Han"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.683"",
    pages = ""5563--5568"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Khoe, Yung Han",Reproducing a Morphosyntactic Tagger with a Meta-{B}i{LSTM} Model over Context Sensitive Token Encodings,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"Reproducibility is generally regarded as being a requirement for any form of experimental science. Even so, reproduction of research results is only recently beginning to be practiced and acknowledged. In the context of the REPROLANG 2020 shared task, we contribute to this trend by reproducing the work reported on by Bohnet et al. (2018) on morphosyntactic tagging. Their meta-BiLSTM model achieved state-of-the-art results across a wide range of languages. This was done by integrating sentence-level and single-word context through synchronized training by a meta-model. Our reproduction only partially confirms the main results of the paper in terms of outperforming earlier models. The results of our reproductions improve on earlier models on the morphological tagging task, but not on the part-of-speech tagging task. Furthermore, even where we improve on earlier models, we fail to match the F1-scores reported for the meta-BiLSTM model. Because we chose not to contact the original authors for our reproduction study, the uncertainty about the degree of parallelism that was achieved between the original study and our reproduction limits the value of our findings as an assessment of the reliability of the original results. At the same time, however, it underscores the relevance of our reproduction effort in regard to the reproducibility and interpretability of those findings. The discrepancies between our findings and the original results demonstrate that there is room for improvement in many aspects of reporting regarding the reproducibility of the experiments. In addition, we suggest that different reporting choices could improve the interpretability of the results.","1. one sentence summary of what paper is about
Reproduction of the morphosyntactic tagging system of Bohnet et al (2018) as a part of the REPROLANG shared task.
2. paper's central goal/question/hypothesis
Is the method of Bohnet et al (2018) reproducible?
3. methodology
Two reproduction attempts: following the parameter description reported in the paper and running source code. Drop some languages for which POS is trivial (the accuracies for those langs can inflate avg scores). Some hyperparameters were changed due to a limited access to resources.
4. reported results
POS tagging gave lower results than initially reported, and lower than previous baselines. (Note that rating difference is tiny--to decimal points.) Morphological tagging reproduction confirms that the system of Bohnet et al is better than previous ones.
5. paper's conclusions and/or main contributions
Partial reproduction of Bohnet et al (AS: note that some parameters were changed, and hard to establish a winner in tagging, since score differences are around 1 point).
",no,"didn't reproduce ablation studies, since the code for them was not available;

chose not to contact authors, but asked some questions via github issues;

published code differed from the paper;

[AS note:] hard to draw a conclusion if reproducible or not from numbers, since accuracies in POS varies in decimal points","according to the author, only partial reproduction: only morphological tagging, but not POS tagging",,,
"@inproceedings{rim-etal-2020-reproducing,
    title = ""Reproducing Neural Ensemble Classifier for Semantic Relation Extraction in{S}cientific Papers"",
    author = ""Rim, Kyeongmin  and
      Tu, Jingxuan  and
      Lynch, Kelley  and
      Pustejovsky, James"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.684"",
    pages = ""5569--5578"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Rim, Kyeongmin  and
      Tu, Jingxuan  and
      Lynch, Kelley  and
      Pustejovsky, James",Reproducing Neural Ensemble Classifier for Semantic Relation Extraction in{S}cientific Papers,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"Within the natural language processing (NLP) community, shared tasks play an important role. They define a common goal and allowthe the comparison of different methods on the same data. SemEval-2018 Task 7 involves the identification and classification of relationsin abstracts from computational linguistics (CL) publications. In this paper we describe an attempt to reproduce the methods and resultsfrom the top performing system at for SemEval-2018 Task 7. We describe challenges we encountered in the process, report on the resultsof our system, and discuss the ways that our attempt at reproduction can inform best practices.","1. one sentence summary of what paper is about
Reproduction of the scientific RE and classification of Rotsztejn et  al. (2018), one of the best systems at SemEval-18, Task 7. Part of REPROLANG.
2. paper's central goal/question/hypothesis
Is the system of Rotsztejn et al (2018) reproducible?
3. methodology
Reimplemented the code. Had to reproduce data collection (the original paper used data augmentation): scraping arXiv for abstracts. 
4. reported results
One subtask was reproduced with 1 point difference in F1. Three other subtasks gave scores lower than the original ones by about 7%. 
5. paper's conclusions and/or main contributions
Reimplemented the system of Rotsztjein et al (2018). Important details were left unknown in the original paper, which hampered reproduction.
",no,"additional data collection, preprocessing and cleaning were not specified in the original paper. Had to guess a lot of details.

how reproduced scores would affect the system rating at SemEval was not covered",partially,,,
"@inproceedings{abdellatif-elgammal-2020-ulmfit,
    title = ""{ULMF}i{T} replication"",
    author = ""Abdellatif, Mohamed  and
      Elgammal, Ahmed"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.685"",
    pages = ""5579--5587"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Abdellatif, Mohamed  and
      Elgammal, Ahmed",{ULMF}i{T} replication,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"In this paper, we reproduce some of the experiments of text classification by fine tuning pre-trained language model on the six English data-sets described in Howard and Ruder (2018) (verification).   Then we investigate applicability of the model as is (pre-trained onEnglish) by conducting additional experiments on three other non-English data-sets that are not in the original paper (extension).  For the verification experiments, we didn’t generate the exact same numbers as the original paper, however, the replication results are in the same range as compared to the baselines reported for comparison purposes. We attribute this to the limitation in computational resources which forced us to run on smaller batch sizes and for fewer number of epochs. Otherwise, we followed in the footsteps of the author to the best of our abilities (e.g. the libraries, tutorials, hyper-parameters and transfer learning methodology). We report implementation details as well as lessons learned in the appendices","1. one sentence summary of what paper is about
This paper reproduces transfer learning results from Howard and Ruder (2018) on 6 English datasets across 3 tasks (Sentiment analysis, Question Classification and Topic Classification) and further extend it to 3 non-English datasets
2. paper's central goal/question/hypothesis
Verify the transfer learning methodolgy by reproducing the results of Howard and Ruder (2018) across the 3 dimensions of reproduciblity. Verified the conclusion based on different findings and values
3. methodology
Used bi-directional Universal Language Model Fine-tuning (ULMFiT) model from the FastAI python library for reproducibility. Also performed an ablation study with uni-directional and not-pretrained model. 
4. reported results
Test error rates (%) reported for all the task comparing the reproduced model with the original paper.  
5. paper's conclusions and/or main contributions
""Verified"" the reproducibility of Howard and Ruder (2018) transfer learning methodology and ""Extended"" the model on non-English datasets. Highlighed the 3 dimensions of reproducibiliy in their results. 
",Followed the definition of replicability and reproducibiltiy by Cohen et al. (2018). Provides extensive details for both replicability and reproducibility while also highlighting the three dimensions of reproduciblitiy in their results,Did not aim for exact replication because of hardware limitataions. Tried to get the results in the same ball park. ,Yes,,,
"@inproceedings{bestgen-2020-reproducing,
    title = ""Reproducing Monolingual, Multilingual and Cross-Lingual {CEFR} Predictions"",
    author = ""Bestgen, Yves"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.687"",
    pages = ""5595--5602"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Bestgen, Yves","Reproducing Monolingual, Multilingual and Cross-Lingual {CEFR} Predictions",Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"This study aims to reproduce the research of Vajjala and Rama (2018) which showed that it is possible to predict the quality of a textwritten by learners of a given language by means of a model built on the basis of texts written by learners of another language.  Theseauthors also pointed out that POStag and dependency n-grams were significantly more effective than text length and global linguisticindices frequently used for this kind of task. The analyses performed show that some important points of their code did not correspond tothe explanations given in the paper. These analyses confirm the possibility to use syntactic n-gram features in cross-lingual experimentsto categorize texts according to their CEFR level (Common European Framework of Reference for Languages).  However, text lengthand  some  classical  indexes  of  readability  are  much  more  effective  in  the  monolingual  and  the  multilingual  experiments  than  whatVajjala and Rama concluded and are even the best performing features when the cross-lingual task is seen as a regression problem.This study emphasized the importance for reproducibility of setting explicitly the reading order of the instances when using a K-foldCV procedure and, more generally, the need to properly randomize these instances before.  It also evaluates a two-step procedure todetermine the degree of statistical significance of the differences observed in a K-fold cross-validation schema and argues against theuse of a Bonferroni-type correction in this context.","1. one sentence summary of what paper is about
Reproduction and analysis of the system of Vajjala and Rama (2018) for automatic essay scoring, which uses supervised learning techniques (SVM, Random Forest, Logistic Regression).
2. paper's central goal/question/hypothesis
Is a cross-lingual classifier possible (i.e. learn a predictive model on one language and test on another) as suggested in Vajjala and Rama?
3. methodology
Reproducing following the original code (two runs with and without docker). Conducted a robust cross-validation study. 
4. reported results
Two runs of reproduction gave different results due to the unstable cross-validation and unrandomised instances. Mixed reproduction results depending on the feature set.  
5. paper's conclusions and/or main contributions
The cross-lingual experiment of Vajjala and Rama was proven right, however some features seem to have more impact than claimed in the initial paper.
",no,F1 calculation (weighted or not) differs in the paper and in the code.,"Partially confirmed the finding.
Hard to tell about the reproduction, since 46 reproduced scores with deviations from -0.11 to +0.24 in F1-score.",,,
"@inproceedings{huber-coltekin-2020-reproduction,
    title = ""Reproduction and Replication: A Case Study with Automatic Essay Scoring"",
    author = {Huber, Eva  and
      {\c{C}}{\""o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.688"",
    pages = ""5603--5613"",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
","Huber, Eva  and
      {\c{C}}{\""o}ltekin, {\c{C}}a{\u{g}}r{\i}",Reproduction and Replication: A Case Study with Automatic Essay Scoring,Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"As in many experimental sciences, reproducibility of experiments has gained ever more attention in the NLP community. This paper presents our reproduction efforts of an earlier study of automatic essay scoring (AES) for determining the proficiency of second language learners in a multilingual setting. We present three sets of experiments with different objectives. First, as prescribed by the LREC 2020 REPROLANG shared task, we rerun the original AES system using the code published by the original authors on the same dataset. Second, we repeat the same experiments on the same data with a different implementation. And third, we test the original system on a different dataset and a different language. Most of our findings are in line with the findings of the original paper. Nevertheless, there are some discrepancies between our results and the results presented in the original paper. We report and discuss these differences in detail. We further go into some points related to confirmation of research findings through reproduction, including the choice of the dataset, reporting and accounting for variability, use of appropriate evaluation metrics, and making code and data available. We also discuss the varying uses and differences between the terms reproduction and replication, and we argue that reproduction, the confirmation of conclusions through independent experiments in varied settings is more valuable than exact replication of the published values.","1. one sentence summary of what paper is about
Reproduction of the automatic essay scoring system of Vajjala and Rama (2018). Part of REPROLANG.
2. paper's central goal/question/hypothesis
Is the system of Vajjala and Rama replicable and reproducible? Can their findings be confirmed on other datasets?
3. methodology
Three methods for recreation: running same code, reimplementing, testing on another language (English). For reimplementation, parameters were optimised, rathern than using libraries' defaults like in the original paper. For testing on English, used the CLC corpus with 12 classes (was also reduced to 5 and 3 in the experiments), which is less heterogeneous than the initial dataset. 
4. reported results
Reproduced scores differ from the original ones, but mostly confirm initial findings. The tuned model yielded better scores.  Some conclusions are not the same, such as the importance of some features, and multilingual transfer. Testing on the CLC gave much lower results due to the different nature of the corpus.
5. paper's conclusions and/or main contributions
Tune your machine learning models. Test on different datasets.
",yes (the whole backround section on repro* terms),"""The overemphasis on exact replication of published  values  may  even  have  unintended  effects,  such  as encouraging researchers to ‘hide’ the natural variation that is expected in the task.""","according to the authors, mostly yes.

AS: High deviations in scores. 46 reproduced scores with deviations from -17 to 22 in F1.",,Long,
"@inproceedings{ballier-etal-2020-learnability,
    title = ""The Learnability of the Annotated Input in {NMT} Replicating (Vanmassenhove and Way, 2018) with {O}pen{NMT}"",
    author = ""Ballier, Nicolas  and
      Amari, Nabil  and
      Merat, Laure  and
      Yun{\`e}s, Jean-Baptiste"",
    booktitle = ""Proceedings of The 12th Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://www.aclweb.org/anthology/2020.lrec-1.691"",
    pages = ""5631--5640"",
    abstract = ""In this paper, we reproduce some of the experiments related to neural network training for Machine Translation as reported in (Vanmassenhove and Way, 2018). They annotated a sample from the EN-FR and EN-DE Europarl aligned corpora with syntactic and semantic annotations to train neural networks with the Nematus Neural Machine Translation (NMT) toolkit. Following the original publication, we obtained lower BLEU scores than the authors of the original paper, but on a more limited set of annotations. In the second half of the paper, we try to analyze the difference in the results obtained and suggest some methods to improve the results. We discuss the Byte Pair Encoding (BPE) used in the pre-processing phase and suggest feature ablation in relation to the granularity of syntactic and semantic annotations. The learnability of the annotated input is discussed in relation to existing resources for the target languages. We also discuss the feature representation likely to have been adopted for combining features."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}","Ballier, Nicolas  and
      Amari, Nabil  and
      Merat, Laure  and
      Yun{\`e}s, Jean-Baptiste","The Learnability of the Annotated Input in {NMT} Replicating (Vanmassenhove and Way, 2018) with {O}pen{NMT}",Proceedings of The 12th Language Resources and Evaluation Conference,2020,LREC,"In this paper, we reproduce some of the experiments related to neural network training for Machine Translation as reported in (Vanmassenhove and Way, 2018). They annotated a sample from the EN-FR and EN-DE Europarl aligned corpora with syntactic and semantic annotations to train neural networks with the Nematus Neural Machine Translation (NMT) toolkit. Following the original publication, we obtained lower BLEU scores than the authors of the original paper, but on a more limited set of annotations. In the second half of the paper, we try to analyze the difference in the results obtained and suggest some methods to improve the results. We discuss the Byte Pair Encoding (BPE) used in the pre-processing phase and suggest feature ablation in relation to the granularity of syntactic and semantic annotations. The learnability of the annotated input is discussed in relation to existing resources for the target languages. We also discuss the feature representation likely to have been adopted for combining features.","1. one sentence summary of what paper is about
Reproduced Vanmassenhove and Way, 2018---the MT system with semantic and syntactic features, and outlined ways to improve the system.
2. paper's central goal/question/hypothesis
Is the method of Vanmassenhove and Way, 2018 reproducible? What improvements would make the system perform better?
3. methodology
Reimplemented the code (original was not available). Had to reannotate the data with CCG and wordnet tags. Introduced additional feature ablation studies.
4. reported results
Rated recreation 4/5 (5 being the most difficult); after the author sent some missing data, the rating went down to 2/5. For the recreated baseline, BLEU is ~11.5 against 21-22 in the original paper.
5. paper's conclusions and/or main contributions
Describe the recreation process of Vanmassenhave and Way, 2018; propose suggestions for improving their MT system.
",no,"The replication was only partial: two scores were reproduced. Not clear: for suggested ablations, didn't run any experiments?

Described their reproducibility effort in terms of the FAIR paradigm (was not very clear for me).",hard to reproduce; BLEU scores lower than reported (-10 points),,,
"@inproceedings{millour-etal-2020-repliquer,
    title = ""R{\'e}pliquer et {\'e}tendre pour l{'}alsacien {``}{\'E}tiquetage en parties du discours de langues peu dot{\'e}es par sp{\'e}cialisation des plongements lexicaux{''} (Replicating and extending for {A}lsatian : {``}{POS} tagging for low-resource languages by adapting word embeddings{''})"",
    author = {Millour, Alice  and
      Fort, Kar{\""e}n  and
      Magistry, Pierre},
    booktitle = ""Actes de la 6e conf{\'e}rence conjointe Journ{\'e}es d'{\'E}tudes sur la Parole (JEP, 33e {\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\'e}dition), Rencontre des {\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\'E}CITAL, 22e {\'e}dition). 2e atelier {\'E}thique et TRaitemeNt Automatique des Langues (ETeRNAL)"",
    month = ""6"",
    year = ""2020"",
    address = ""Nancy, France"",
    publisher = ""ATALA et AFCP"",
    url = ""https://www.aclweb.org/anthology/2020.jeptalnrecital-eternal.4"",
    pages = ""29--37"",
    language = ""French"",
}
","Millour, Alice  and
      Fort, Kar{\""e}n  and
      Magistry, Pierre",R{\'e}pliquer et {\'e}tendre pour l{'}alsacien {``}{\'E}tiquetage en parties du discours de langues peu dot{\'e}es par sp{\'e}cialisation des plongements lexicaux{''} (Replicating and extending for {A}lsatian : {``}{POS} tagging for low-resource languages by adapting word embeddings{''}),"Actes de la 6e conf{\'e}rence conjointe Journ{\'e}es d'{\'E}tudes sur la Parole (JEP, 33e {\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\'e}dition), Rencontre des {\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\'E}CITAL, 22e {\'e}dition). 2e atelier {\'E}thique et TRaitemeNt Automatique des Langues (ETeRNAL)",2020,WS,"We present here the results of our efforts in replicating and extending for Alsatian an experiment concerning the POS tagging of low-resourced languages by adapting word embeddings (Magistry et al., 2018). This work was performed in close collaboration with the authors of the original article. This rich interaction allowed us to identify the missing elements in the presentation of the experiment, to add them and to extend the experiment to robustness to variation.","1. one sentence summary of what paper is about
This short paper describes the authors' attempt to get a version of the POS tagger for Alstian reported by Magistry et al. to work with the same accuracy as reported in the earlier paper
2. paper's central goal/question/hypothesis
Can the the earlier POS tagger be recreated with the same accuracy?
3. methodology
In collaboration with the authors of the orginal paper, try to recreate the tagger as faithfully as possible, using resources provided by the original authors and recreating some where necessary.
4. reported results
Found the documentation of what was done in the original work very incomplete and did not manage to reproduce the very high 0.91 acc result from the earlier paper (highest attempt yielded 0.87). 
5. paper's conclusions and/or main contributions
Precise documentation about implementations and system and dataset versions is crucial for reproduction attempts.
","* One result (0.78) on the smallest version of the corpus is described as being reproduced but it's not clear what it was compared to as original paper does not have that number in it.
* this is in French, as is the original paper",,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,5,Left to review,,,,
,,,,,,,,10,AB,,,,
,,,,,,,,8,AS,,,,
,,,,,,,,12,SA,,,,